{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence To Sequence(Seq2Seq)\n",
    "### Encoder\n",
    "\n",
    "![](img/Encoder001.png)\n",
    "相关方法\n",
    "- residual:   inputs + （通过一层的outputs）\n",
    "- Layer Normalization:   $x_i^{'} = \\frac{x_i-m}{\\sigma}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "#### 自回归\n",
    "![](img/Decoder001.png)\n",
    "#### END输出\n",
    "![](img/Decoder002.png)\n",
    "\n",
    "#### 非-自回归 Non AutoRegressive\n",
    "![](img/Decoder003.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossAttention - Decoder与Encoder之间的联系\n",
    "![](img/crossAttention.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何训练Transformer\n",
    "![](img/Transformer001.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
