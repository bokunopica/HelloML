{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self-attention层\n",
    "![](img/selfAttention001.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以b1为例 输入通过self_attention层如何得到b1\n",
    "\n",
    "#### 计算两个输入的相关性$\\alpha$ - Dot-product方法\n",
    "![](img/selfAttention002.png)\n",
    "\n",
    "![](img/selfAttention003.png)\n",
    "\n",
    "1. 通过$q^1以及k^n$外带一层激活函数来求得相应的$\\alpha_{1,i}^{'}$\n",
    "  - $\\alpha_{1,i}^{'} = exp(a_{1,i}/\\sum_i exp(\\alpha_{1,j}))$ \n",
    "2. 通过$\\alpha^{'}$以及$v^i$来计算$b^1$\n",
    "  - $b^1 = \\sum_i \\alpha_{1,i}^{'}*v^i$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整的self-attention步骤\n",
    "![](img/selfAttention004.png)\n",
    "![](img/selfAttention005.png)\n",
    "![](img/selfAttention006.png)\n",
    "![](img/selfAttention007.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## masked self-attention\n",
    "计算时只考虑前面的数据关系\n",
    "![](img/selfAttention010.png)\n",
    "![](img/selfAttention011.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecurrentNeuralNetwork\n",
    "#### 1-of-N encoding\n",
    "![](img/1ofn.png)\n",
    "#### beyond 1-of-N encoding\n",
    "![](img/wordHashing.png)\n",
    "\n",
    "#### RNN\n",
    "![](img/rnn01.png)\n",
    "\n",
    "#### Long Short-term Memory(LSTM)\n",
    "![](img/lstm.png)\n",
    "![](img/lstm1.png)\n",
    "\n",
    "\n",
    "#### LSTM-EXAMPLE\n",
    "![](img/lstm2.png)\n",
    "\n",
    "#### LSTM-Simplified\n",
    "![](img/lstm3.png)\n",
    "![](img/lstm4.png)\n",
    "\n",
    "#### LSTM-Full\n",
    "![](img/lstm5.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
