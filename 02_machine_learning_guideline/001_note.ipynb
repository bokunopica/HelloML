{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general guide\n",
    "![优化算法的攻略](../img/general_guide_of_DL.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习组成部分\n",
    "1. Model\n",
    "   1. LinearRegression\n",
    "   2. NeuralNetwork\n",
    "   3. LogisticRegression\n",
    "      - sigmoid\n",
    "   4. Classification\n",
    "      - softmax\n",
    "   5. ConvolutionalNeuralNetwork\n",
    "   6. ......\n",
    "2. Loss\n",
    "   - Loss = $\\frac{1}{N}\\sum_n e_n$\n",
    "   - ways of calculate loss\n",
    "     - MeanSquareError\n",
    "        - $e=\\sum_i (\\hat y_i - y_i^,)^2$\n",
    "     - Cross-entropy\n",
    "        - $e=-\\sum_i \\hat y_i\\ln y_i^,$ \n",
    "        - minimise cross-entropy = maximizing likelihood(极大似然估计)\n",
    "        - usually used with softmax function together\n",
    "3. Optimizer\n",
    "   1. GradientDescent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降可能遇到的问题\n",
    "1. 鞍点\n",
    "    - 导数为0的点(驻点 critical point)不一定为极值点\n",
    "    - 通过计算hession矩阵的特征值$\\lambda$来确定是否为极小值点\n",
    "      - $\\lambda$有正有负 鞍点\n",
    "      - $\\lambda$均为负 极大值点\n",
    "      - $\\lambda$均为正 极小值点 成立\n",
    "    - 逃离方法\n",
    "      - 根据负特征值的特征向量来调整(实际很少使用 很难算)\n",
    "      - Small Batch\n",
    "      - Momentum\n",
    "2. 梯度学习率\n",
    "    - Loss变得很低后 梯度值在一个范围震荡\n",
    "    - 采用Adaptive Learning Rate来调整LR值使得损失函数能够达到最小值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch\n",
    "- hyperParameter\n",
    "- 完整数据集可以分为若干个Batch，1次epoch中对应了若干个update， 每个update对应一个batch\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Small Batch</th>\n",
    "        <th>Large Batch</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>speed for one update(no parallel)</td>\n",
    "        <td>Faster</td>\n",
    "        <td>Slow</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>speed for one update(with parallel)</td>\n",
    "        <td>Same</td>\n",
    "        <td>Same(not too large)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>time for one epoch</td>\n",
    "        <td>Slower</td>\n",
    "        <td>Faster</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gradient</td>\n",
    "        <td>Noisy</td>\n",
    "        <td>Stable</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Optimization</td>\n",
    "        <td>Better</td>\n",
    "        <td>Worse</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Generalization</td>\n",
    "        <td>Better</td>\n",
    "        <td>Worse</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "- hyperParameter\n",
    "- 梯度下降的惯性\n",
    "- movement with momentum: movement of last step minus gradius at present\n",
    "- 每一次梯度下降时都需要累加上一步梯度下降的值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rate\n",
    "\n",
    "### Root Mean Square方法(used in Adagrad)\n",
    "\n",
    "$\\theta_i^1=\\theta_i^0-\\frac{\\eta}{\\sigma_i^0}*g_i^0$, $\\sigma_i^0=\\sqrt{(g_i^0)^2}=|g_i^0|$ \n",
    "\n",
    "$\\theta_i^2=\\theta_i^1-\\frac{\\eta}{\\sigma_i^1}*g_i^1$, $\\sigma_i^1=\\sqrt{\\frac{1}{2}[(g_i^0)^2+(g_i^1)^2]}$ \n",
    "\n",
    "$\\theta_i^3=\\theta_i^2-\\frac{\\eta}{\\sigma_i^2}*g_i^2$, $\\sigma_i^2=\\sqrt{\\frac{1}{3}[(g_i^0)^2+(g_i^1)^2+(g_i^1)^3]}$ \n",
    "\n",
    "......\n",
    "\n",
    "$\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}*g_i^t$, $\\sigma_i^t=\\sqrt{\\frac{1}{t+1}\\sum_{i=0}^t(g_i^t)^2}$ \n",
    "\n",
    "\n",
    "### RMSProp方法\n",
    "- hyperParameter: $\\alpha \\quad (0<\\alpha<1)$ 当前梯度对学习率的影响因子\n",
    "- 梯度较低时 $\\sigma_i^t$较小，则$\\frac{\\eta}{\\sigma_i^t}$(学习率)较大, 反之同理\n",
    "\n",
    "$\\theta_i^1=\\theta_i^0-\\frac{\\eta}{\\sigma_i^0}*g_i^0$, $\\sigma_i^0=\\sqrt{(g_i^0)^2}=|g_i^0|$ \n",
    "\n",
    "$\\theta_i^2=\\theta_i^1-\\frac{\\eta}{\\sigma_i^1}*g_i^1$, $\\sigma_i^1=\\sqrt{\\alpha(\\sigma_i^0)^2+(1-\\alpha)(g_i^1)^2}$ \n",
    "\n",
    "$\\theta_i^3=\\theta_i^2-\\frac{\\eta}{\\sigma_i^2}*g_i^2$, $\\sigma_i^2=\\sqrt{\\alpha(\\sigma_i^1)^2+(1-\\alpha)(g_i^2)^2}$ \n",
    "\n",
    "......\n",
    "\n",
    "$\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}*g_i^t$, $\\sigma_i^t=\\sqrt{\\alpha(\\sigma_i^{t-1})^2+(1-\\alpha)(g_i^t)^2}$ \n",
    "\n",
    "### Learning Rate Scheduling\n",
    "- Learning Rate Decay: 学习率随时间衰减\n",
    "- Warm Up: Increase and then decrease"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer: RMSDrop + Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Optimization\n",
    "### (Vanilla) Gradient Descent\n",
    "$\\theta_i^t+1 \\leftarrow \\theta_i^t-\\eta g_i^t$\n",
    "\n",
    "### Various Imporvements\n",
    "$\\theta_i^t+1 \\leftarrow \\theta_i^t-\\frac{\\eta^t}{\\sigma_i^t}m_i^t$\n",
    "- $\\eta^t$--Learning Rate Decay or Warm Up\n",
    "- $\\sigma_i^t$--Root Mean Square\n",
    "- $m_i^t$--momentum"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
