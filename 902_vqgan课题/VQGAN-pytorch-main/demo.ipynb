{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import requests\n",
    "from torchvision import utils as vutils\n",
    "from torchvision.models import vgg16\n",
    "from helper import ResidualBlock, NonLocalBlock, DownSampleBlock, UpSampleBlock, GroupNorm, Swish\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        channels = [128, 128, 128, 256, 256, 512]\n",
    "        attn_resolutions = [16]\n",
    "        num_res_blocks = 2\n",
    "        resolution = 256\n",
    "        layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
    "        for i in range(len(channels)-1):\n",
    "            in_channels = channels[i]\n",
    "            out_channels = channels[i + 1]\n",
    "            for j in range(num_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attn_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            if i != len(channels)-2:\n",
    "                layers.append(DownSampleBlock(channels[i+1]))\n",
    "                resolution //= 2\n",
    "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "        layers.append(NonLocalBlock(channels[-1]))\n",
    "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "        layers.append(GroupNorm(channels[-1]))\n",
    "        layers.append(Swish())\n",
    "        layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()\n",
    "        channels = [512, 256, 256, 128, 128]\n",
    "        attn_resolutions = [16]\n",
    "        num_res_blocks = 3\n",
    "        resolution = 16\n",
    "\n",
    "        in_channels = channels[0]\n",
    "        layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
    "                  ResidualBlock(in_channels, in_channels),\n",
    "                  NonLocalBlock(in_channels),\n",
    "                  ResidualBlock(in_channels, in_channels)]\n",
    "\n",
    "        for i in range(len(channels)):\n",
    "            out_channels = channels[i]\n",
    "            for j in range(num_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attn_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            if i != 0:\n",
    "                layers.append(UpSampleBlock(in_channels))\n",
    "                resolution *= 2\n",
    "\n",
    "        layers.append(GroupNorm(in_channels))\n",
    "        layers.append(Swish())\n",
    "        layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Codebook(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Codebook, self).__init__()\n",
    "        self.num_codebook_vectors = args.num_codebook_vectors\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.beta = args.beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.latent_dim)\n",
    "\n",
    "        # (z_flattened-embedding.weight)^2\n",
    "        # z_flat [1024, 256] embedding [1024, 256] 要获得1024,1024大小得(a-b)^2\n",
    "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
    "            2*(torch.matmul(z_flattened, self.embedding.weight.t()))\n",
    "\n",
    "        min_encoding_indices = torch.argmin(d, dim=1)\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
    "\n",
    "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n",
    "\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        z_q = z_q.permute(0, 3, 1, 2)\n",
    "\n",
    "        return z_q, min_encoding_indices, loss\n",
    "    \n",
    "\n",
    "\n",
    "class VQGAN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VQGAN, self).__init__()\n",
    "        self.encoder = Encoder(args).to(device=args.device)\n",
    "        self.decoder = Decoder(args).to(device=args.device)\n",
    "        self.codebook = Codebook(args).to(device=args.device)\n",
    "        self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
    "        self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "        post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n",
    "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
    "\n",
    "        return decoded_images, codebook_indices, q_loss\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "        return codebook_mapping, codebook_indices, q_loss\n",
    "\n",
    "    def decode(self, z):\n",
    "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
    "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
    "        return decoded_images\n",
    "\n",
    "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
    "        last_layer = self.decoder.model[-1]\n",
    "        last_layer_weight = last_layer.weight\n",
    "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
    "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
    "\n",
    "        λ = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
    "        λ = torch.clamp(λ, 0, 1e4).detach()\n",
    "        return 0.8 * λ\n",
    "\n",
    "    @staticmethod\n",
    "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
    "        if i < threshold:\n",
    "            disc_factor = value\n",
    "        return disc_factor\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer(\"shift\", torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.shift) / self.scale\n",
    "    \n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg_pretrained_features = vgg16(pretrained=True).features\n",
    "        slices = [vgg_pretrained_features[i] for i in range(30)]\n",
    "        self.slice1 = nn.Sequential(*slices[0:4])\n",
    "        self.slice2 = nn.Sequential(*slices[4:9])\n",
    "        self.slice3 = nn.Sequential(*slices[9:16])\n",
    "        self.slice4 = nn.Sequential(*slices[16:23])\n",
    "        self.slice5 = nn.Sequential(*slices[23:30])\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.slice1(x)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        return vgg_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
    "    \n",
    "\n",
    "\n",
    "URL_MAP = {\n",
    "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
    "}\n",
    "\n",
    "CKPT_MAP = {\n",
    "    \"vgg_lpips\": \"vgg.pth\"\n",
    "}\n",
    "\n",
    "\n",
    "def download(url, local_path, chunk_size=1024):\n",
    "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for data in r.iter_content(chunk_size=chunk_size):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        pbar.update(chunk_size)\n",
    "\n",
    "def get_ckpt_path(name, root):\n",
    "    assert name in URL_MAP\n",
    "    path = os.path.join(root, CKPT_MAP[name])\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {name} model from {URL_MAP[name]} to {path}\")\n",
    "        download(URL_MAP[name], path)\n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "def norm_tensor(x):\n",
    "    \"\"\"\n",
    "    Normalize images by their length to make them unit vector?\n",
    "    :param x: batch of images\n",
    "    :return: normalized batch of images\n",
    "    \"\"\"\n",
    "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + 1e-10)\n",
    "\n",
    "\n",
    "\n",
    "def spatial_average(x):\n",
    "    \"\"\"\n",
    "     imgs have: batch_size x channels x width x height --> average over width and height channel\n",
    "    :param x: batch of images\n",
    "    :return: averaged images along width and height\n",
    "    \"\"\"\n",
    "    return x.mean([2, 3], keepdim=True)\n",
    "\n",
    "    \n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LPIPS, self).__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        self.channels = [64, 128, 256, 512, 512]\n",
    "        self.vgg = VGG16()\n",
    "        self.lins = nn.ModuleList([\n",
    "            NetLinLayer(self.channels[0]),\n",
    "            NetLinLayer(self.channels[1]),\n",
    "            NetLinLayer(self.channels[2]),\n",
    "            NetLinLayer(self.channels[3]),\n",
    "            NetLinLayer(self.channels[4])\n",
    "        ])\n",
    "\n",
    "        self.load_from_pretrained()\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
    "        ckpt = get_ckpt_path(name, \"vgg_lpips\")\n",
    "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "\n",
    "    def forward(self, real_x, fake_x):\n",
    "        features_real = self.vgg(self.scaling_layer(real_x))\n",
    "        features_fake = self.vgg(self.scaling_layer(fake_x))\n",
    "        diffs = {}\n",
    "\n",
    "        for i in range(len(self.channels)):\n",
    "            diffs[i] = (norm_tensor(features_real[i]) - norm_tensor(features_fake[i])) ** 2\n",
    "\n",
    "        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n",
    "    \n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args, num_filters_last=64, n_layers=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(args.image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
    "        num_filters_mult = 1\n",
    "\n",
    "        for i in range(1, n_layers + 1):\n",
    "            num_filters_mult_last = num_filters_mult\n",
    "            num_filters_mult = min(2 ** i, 8)\n",
    "            layers += [\n",
    "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
    "                          2 if i < n_layers else 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePaths(Dataset):\n",
    "    def __init__(self, path, size=None):\n",
    "        self.size = size\n",
    "\n",
    "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
    "        self._length = len(self.images)\n",
    "\n",
    "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
    "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
    "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.preprocess_image(self.images[i])\n",
    "        return example\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    \"\"\"\n",
    "    读取训练数据\n",
    "    \"\"\"\n",
    "    train_data = ImagePaths(args.dataset_path, size=256)\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def load_rand_data():\n",
    "    # train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    # return test_loader\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainVQGAN:\n",
    "    def __init__(self, args):\n",
    "        self.vqgan = VQGAN(args).to(device=args.device)\n",
    "        self.discriminator = Discriminator(args).to(device=args.device)\n",
    "        self.discriminator.apply(weights_init)\n",
    "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
    "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
    "\n",
    "        self.prepare_training()\n",
    "\n",
    "        self.train(args)\n",
    "\n",
    "    def configure_optimizers(self, args):\n",
    "        lr = args.learning_rate\n",
    "        opt_vq = torch.optim.Adam(\n",
    "            list(self.vqgan.encoder.parameters()) +\n",
    "            list(self.vqgan.decoder.parameters()) +\n",
    "            list(self.vqgan.codebook.parameters()) +\n",
    "            list(self.vqgan.quant_conv.parameters()) +\n",
    "            list(self.vqgan.post_quant_conv.parameters()),\n",
    "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
    "        )\n",
    "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
    "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
    "\n",
    "        return opt_vq, opt_disc\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_training():\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    def train(self, args):\n",
    "        train_dataset = load_data(args)\n",
    "        steps_per_epoch = len(train_dataset)\n",
    "        for epoch in range(args.epochs):\n",
    "            with tqdm(range(len(train_dataset))) as pbar:\n",
    "                for i, imgs in zip(pbar, train_dataset):\n",
    "                    imgs = imgs.to(device=args.device)\n",
    "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
    "\n",
    "                    disc_real = self.discriminator(imgs)\n",
    "                    disc_fake = self.discriminator(decoded_images)\n",
    "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i, threshold=args.disc_start)\n",
    "\n",
    "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
    "                    rec_loss = torch.abs(imgs - decoded_images)\n",
    "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
    "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
    "                    g_loss = -torch.mean(disc_fake)\n",
    "\n",
    "                    λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
    "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
    "\n",
    "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
    "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
    "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
    "\n",
    "                    self.opt_vq.zero_grad()\n",
    "                    vq_loss.backward(retain_graph=True)\n",
    "\n",
    "                    self.opt_disc.zero_grad()\n",
    "                    gan_loss.backward()\n",
    "\n",
    "                    self.opt_vq.step()\n",
    "                    self.opt_disc.step()\n",
    "\n",
    "                    if i % 10 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
    "                            vutils.save_image(real_fake_images, os.path.join(\"results\", f\"{epoch}_{i}.jpg\"), nrow=4)\n",
    "\n",
    "                    pbar.set_postfix(\n",
    "                        VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
    "                        GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
    "                    )\n",
    "                    pbar.update(0)\n",
    "                # torch.save(self.vqgan.state_dict(), os.path.join(\"checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n",
    "                # 100次epoch存一次模型 训练5000个epoch磁盘不够吃\n",
    "                # if((epoch+1)%100==0):\n",
    "                #     torch.save(self.vqgan.state_dict(), os.path.join(\"checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n",
    "                #     torch.save(self.vqgan.state_dict(), os.path.join(\"checkpoints\", f\"vqgan_last_ckpt.pt\"))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyCodes\\HelloML\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\MyCodes\\HelloML\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.14it/s, GAN_Loss=0, VQ_Loss=0.518]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.26it/s, GAN_Loss=0, VQ_Loss=0.464]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.20it/s, GAN_Loss=0, VQ_Loss=0.451]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.94it/s, GAN_Loss=0, VQ_Loss=0.456]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.04it/s, GAN_Loss=0, VQ_Loss=0.421]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.08it/s, GAN_Loss=0, VQ_Loss=0.418]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.31it/s, GAN_Loss=0, VQ_Loss=0.41]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.34it/s, GAN_Loss=0, VQ_Loss=0.435]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.24it/s, GAN_Loss=0, VQ_Loss=0.41]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.22it/s, GAN_Loss=0, VQ_Loss=0.464]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.32it/s, GAN_Loss=0, VQ_Loss=0.465]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.34it/s, GAN_Loss=0, VQ_Loss=0.436]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.83it/s, GAN_Loss=0, VQ_Loss=0.482]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.18it/s, GAN_Loss=0, VQ_Loss=0.543]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.13it/s, GAN_Loss=0, VQ_Loss=0.589]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.88it/s, GAN_Loss=0, VQ_Loss=0.679]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.13it/s, GAN_Loss=0, VQ_Loss=0.768]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.11it/s, GAN_Loss=0, VQ_Loss=0.917]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.07it/s, GAN_Loss=0, VQ_Loss=1.02]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.08it/s, GAN_Loss=0, VQ_Loss=1.21]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.10it/s, GAN_Loss=0, VQ_Loss=1.34]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.87it/s, GAN_Loss=0, VQ_Loss=1.51]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.13it/s, GAN_Loss=0, VQ_Loss=1.6]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.62it/s, GAN_Loss=0, VQ_Loss=1.84]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.87it/s, GAN_Loss=0, VQ_Loss=1.98]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.01it/s, GAN_Loss=0, VQ_Loss=2.15]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.14it/s, GAN_Loss=0, VQ_Loss=2.2]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.13it/s, GAN_Loss=0, VQ_Loss=2.19]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.89it/s, GAN_Loss=0, VQ_Loss=2.21]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.10it/s, GAN_Loss=0, VQ_Loss=2.19]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.88it/s, GAN_Loss=0, VQ_Loss=2.13]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.14it/s, GAN_Loss=0, VQ_Loss=2.16]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.00it/s, GAN_Loss=0, VQ_Loss=2.21]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.06it/s, GAN_Loss=0, VQ_Loss=2.26]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.08it/s, GAN_Loss=0, VQ_Loss=2.19]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.91it/s, GAN_Loss=0, VQ_Loss=2.26]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.07it/s, GAN_Loss=0, VQ_Loss=2.23]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.98it/s, GAN_Loss=0, VQ_Loss=2.25]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.26it/s, GAN_Loss=0, VQ_Loss=2.26]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.20it/s, GAN_Loss=0, VQ_Loss=2.24]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.02it/s, GAN_Loss=0, VQ_Loss=2.18]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.09it/s, GAN_Loss=0, VQ_Loss=2.21]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.07it/s, GAN_Loss=0, VQ_Loss=2.16]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.04it/s, GAN_Loss=0, VQ_Loss=2.18]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.11it/s, GAN_Loss=0, VQ_Loss=2.25]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.12it/s, GAN_Loss=0, VQ_Loss=2.21]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.94it/s, GAN_Loss=0, VQ_Loss=2.31]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.11it/s, GAN_Loss=0, VQ_Loss=2.33]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.91it/s, GAN_Loss=0, VQ_Loss=2.25]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.11it/s, GAN_Loss=0, VQ_Loss=2.17]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "vqgan 训练\n",
    "\"\"\"\n",
    "args = Args()\n",
    "args.latent_dim = 256\n",
    "args.image_size = 256\n",
    "args.num_codebook_vectors = 1024\n",
    "args.beta = 0.25\n",
    "args.image_channels = 3\n",
    "args.dataset_path = r\"../chex/\"\n",
    "args.device = \"cuda\"\n",
    "args.batch_size = 1\n",
    "args.epochs = 50\n",
    "args.learning_rate = 2.25e-05\n",
    "args.beta1 = 0.5\n",
    "args.beta2 = 0.9\n",
    "args.disc_start = 5000\n",
    "args.disc_factor = 1.\n",
    "args.rec_loss_factor = 1\n",
    "args.perceptual_loss_factor = 1.\n",
    "\n",
    "\n",
    "train_vqgan = TrainVQGAN(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "图像生成\n",
    "\"\"\"\n",
    "args = Args()\n",
    "args.latent_dim = 256\n",
    "args.image_size = 256\n",
    "args.num_codebook_vectors = 1024\n",
    "args.beta = 0.25\n",
    "args.image_channels = 3\n",
    "args.dataset_path = r\"../chex/\"\n",
    "args.device = \"cuda\"\n",
    "args.batch_size = 1\n",
    "args.epochs = 10000\n",
    "args.learning_rate = 2.25e-05\n",
    "args.beta1 = 0.5\n",
    "args.beta2 = 0.9\n",
    "args.disc_start = 5000\n",
    "args.disc_factor = 1.\n",
    "args.rec_loss_factor = 1\n",
    "args.perceptual_loss_factor = 1.\n",
    "\n",
    "model = train_vqgan.vqgan\n",
    "dataset = load_data(args)\n",
    "\n",
    "\n",
    "\n",
    "with tqdm(range(len(dataset))) as pbar:\n",
    "    for i, imgs in zip(pbar, dataset):\n",
    "        # imgs = torch.zeros(size=[1,3,256,256])\n",
    "        imgs = imgs.to(device=args.device)\n",
    "        decoded_images, _, q_loss = model(imgs)\n",
    "        vutils.save_image(decoded_images.add(1).mul(0.5)[:4], os.path.join(\"demo_results\", f\"001_demo.jpg\"), nrow=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
